<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/207c0470b180c9e7.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-ca2f87e3a88850ff.js"/><script src="/_next/static/chunks/4bd1b696-2581ea8f93aa337b.js" async=""></script><script src="/_next/static/chunks/684-4c102cf6d11d20bb.js" async=""></script><script src="/_next/static/chunks/main-app-f4365aec3eb4723f.js" async=""></script><script src="/_next/static/chunks/63-817a1b282d253a1a.js" async=""></script><script src="/_next/static/chunks/app/layout-fdd88cdb4d505173.js" async=""></script><script src="/_next/static/chunks/app/publications/page-24d38f64d1e5a0e1.js" async=""></script><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_5cfdac __variable_9a8899 antialiased"><nav class="fixed top-0 left-0 w-full z-30 bg-gradient-to-r from-black/90 via-blue-950/80 to-yellow-900/70 backdrop-blur-2xl border-b-2 border-blue-400/30 shadow-2xl shadow-blue-900/30 animate-fade-in-down"><div class="flex items-center gap-4 md:gap-10 px-4 md:px-10 py-5"><span class="text-white text-3xl font-extrabold tracking-widest mr-6 md:mr-14 select-none flex items-center gap-3 drop-shadow-lg"><span class="bg-gradient-to-r from-yellow-400 via-blue-400 to-blue-700 bg-clip-text text-transparent animate-gradient-x">GJ</span></span><button class="md:hidden ml-auto text-white focus:outline-none" aria-label="Open menu"><svg width="32" height="32" fill="none" viewBox="0 0 24 24"><path stroke="currentColor" stroke-width="2" stroke-linecap="round" d="M4 7h16M4 12h16M4 17h16"></path></svg></button><div class="hidden md:flex items-center gap-10 flex-1"><a class="block relative text-lg tracking-widest pb-1 transition-all duration-200 my-2 md:my-0 after:absolute after:left-0 after:-bottom-1 after:w-full after:h-1 after:bg-gradient-to-r after:from-yellow-400 after:to-blue-400 after:rounded-full text-blue-200 font-semibold hover:text-yellow-300 hover:scale-110 after:opacity-0" href="/">HOME</a><a class="block relative text-lg tracking-widest pb-1 transition-all duration-200 my-2 md:my-0 after:absolute after:left-0 after:-bottom-1 after:w-full after:h-1 after:bg-gradient-to-r after:from-yellow-400 after:to-blue-400 after:rounded-full text-blue-200 font-semibold hover:text-yellow-300 hover:scale-110 after:opacity-0" href="/cv">CV</a><a class="block relative text-lg tracking-widest pb-1 transition-all duration-200 my-2 md:my-0 after:absolute after:left-0 after:-bottom-1 after:w-full after:h-1 after:bg-gradient-to-r after:from-yellow-400 after:to-blue-400 after:rounded-full text-blue-200 font-semibold hover:text-yellow-300 hover:scale-110 after:opacity-0" href="/bio">BIO</a><a class="block relative text-lg tracking-widest pb-1 transition-all duration-200 my-2 md:my-0 after:absolute after:left-0 after:-bottom-1 after:w-full after:h-1 after:bg-gradient-to-r after:from-yellow-400 after:to-blue-400 after:rounded-full text-white font-bold border-b-2 border-blue-400 after:opacity-100" href="/publications">PUBLICATIONS</a><a class="block relative text-lg tracking-widest pb-1 transition-all duration-200 my-2 md:my-0 after:absolute after:left-0 after:-bottom-1 after:w-full after:h-1 after:bg-gradient-to-r after:from-yellow-400 after:to-blue-400 after:rounded-full text-blue-200 font-semibold hover:text-yellow-300 hover:scale-110 after:opacity-0" href="/blog">BLOG</a><div class="flex items-center gap-5 mt-2 md:mt-0"><a href="https://scholar.google.com/citations?user=qsIjwG4AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer" class="hover:scale-125 transition-transform duration-200 group"><img alt="GitHub" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" class="w-8 h-8 opacity-80 group-hover:opacity-100 drop-shadow-md group-hover:drop-shadow-blue-400 animate-bounce" style="color:transparent" src="/globe.svg"/></a><a href="mailto:gaganjain1582@microsoft.com" class="hover:scale-125 transition-transform duration-200 group"><img alt="Email" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" class="w-8 h-8 opacity-80 group-hover:opacity-100 drop-shadow-md group-hover:drop-shadow-yellow-400 animate-pulse" style="color:transparent" src="/vercel.svg"/></a></div></div></div></nav><div style="padding-top:90px"><div class="min-h-screen flex flex-col relative overflow-hidden bg-black"><main class="min-h-screen flex flex-col items-center justify-start bg-black/90 px-4 pt-32 pb-16"><h1 class="text-4xl md:text-5xl font-extrabold text-blue-400 mb-10 tracking-tight">All Publications</h1><section class="w-full max-w-3xl mb-12 animate-fade-in-up delay-400"><div class="flex flex-wrap items-center gap-8 bg-gradient-to-br from-blue-900/60 to-yellow-900/40 rounded-2xl p-8 shadow-2xl border border-blue-400/20 backdrop-blur-md mb-8"><img alt="Masked Generative Nested Transformers with Decode Time Scaling teaser" loading="lazy" width="224" height="126" decoding="async" data-nimg="1" class="w-56 rounded-xl shadow-md mb-4 flex-shrink-0 border-2 border-blue-400/30" style="color:transparent" src="/fastgen.png"/><div class="flex-1 min-w-[220px]"><div class="text-blue-300 font-extrabold text-xl mb-1">Masked Generative Nested Transformers with Decode Time Scaling</div><div class="text-gray-200 font-semibold mb-1">Sahil Goyal, Debapriya Tula, Pradeep Shenoy, Prateek Jain, Sujoy Paul</div><div class="mb-2 flex flex-wrap gap-2 items-center"><span class="text-blue-300 font-bold">ICML 2025</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Vision</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Efficiency</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Conditional Computation</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Selected Papers</span></div><div class="text-gray-300 mb-2"><b>TL;DR:</b> <!-- -->An efficient framework for progressive decoding with nested models for faster inference.</div><div class="flex flex-wrap gap-2 mb-2"><a href="https://arxiv.org/abs/2407.19985" class="font-bold text-white bg-gradient-to-r from-blue-600 to-blue-400 rounded-lg px-5 py-2 shadow hover:from-blue-700 hover:to-blue-500 transition" target="_blank" rel="noopener noreferrer">Read Paper</a><a href="https://github.com/yanndubs/invariant-self-supervised-learning" class="font-bold text-blue-300 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition" target="_blank" rel="noopener noreferrer">Code</a><a href="https://x.com/gaganjain1582/status/1820107343369035819" class="font-bold text-blue-400 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition" target="_blank" rel="noopener noreferrer">Twitter</a></div><div class="text-gray-400 text-xs">2024-11-15</div></div></div><div class="flex flex-wrap items-center gap-8 bg-gradient-to-br from-blue-900/60 to-yellow-900/40 rounded-2xl p-8 shadow-2xl border border-blue-400/20 backdrop-blur-md mb-8"><img alt="Bayesian Collaborative Bandits with Thompson Sampling for Improved Outreach in Maternal Health teaser" loading="lazy" width="224" height="126" decoding="async" data-nimg="1" class="w-56 rounded-xl shadow-md mb-4 flex-shrink-0 border-2 border-blue-400/30" style="color:transparent" src="/eluder.png"/><div class="flex-1 min-w-[220px]"><div class="text-blue-300 font-extrabold text-xl mb-1">Bayesian Collaborative Bandits with Thompson Sampling for Improved Outreach in Maternal Health</div><div class="text-gray-200 font-semibold mb-1">Arpan Dasgupta, Arun Suggala, Karthikeyan Shanmugam, Milind Tambe, Aparna Taneja</div><div class="mb-2 flex flex-wrap gap-2 items-center"><span class="text-blue-300 font-bold">AAMAS 2025</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Reinforcement Learning</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Theory</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Societal Impact</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Selected Papers</span></div><div class="text-gray-300 mb-2"><b>TL;DR:</b> <!-- -->An improved collaborative bandits approach with bayesian regret derivation for a special case!</div><div class="flex flex-wrap gap-2 mb-2"><a href="https://arxiv.org/abs/2410.21405" class="font-bold text-white bg-gradient-to-r from-blue-600 to-blue-400 rounded-lg px-5 py-2 shadow hover:from-blue-700 hover:to-blue-500 transition" target="_blank" rel="noopener noreferrer">Read Paper</a><a href="https://github.com/yanndubs/invariant-self-supervised-learning" class="font-bold text-blue-300 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition" target="_blank" rel="noopener noreferrer">Code</a><a href="https://x.com/gaganjain1582/status/1820107343369035819" class="font-bold text-blue-400 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition" target="_blank" rel="noopener noreferrer">Twitter</a></div><div class="text-gray-400 text-xs">2024-10-11</div></div></div><div class="flex flex-wrap items-center gap-8 bg-gradient-to-br from-blue-900/60 to-yellow-900/40 rounded-2xl p-8 shadow-2xl border border-blue-400/20 backdrop-blur-md mb-8"><img alt="Mixture of Nested Experts: Adaptive Processing of Visual Tokens teaser" loading="lazy" width="224" height="126" decoding="async" data-nimg="1" class="w-56 rounded-xl shadow-md mb-4 flex-shrink-0 border-2 border-blue-400/30" style="color:transparent" src="/mone.png"/><div class="flex-1 min-w-[220px]"><div class="text-blue-300 font-extrabold text-xl mb-1">Mixture of Nested Experts: Adaptive Processing of Visual Tokens</div><div class="text-gray-200 font-semibold mb-1">Nidhi Hegde, Aditya Kusupati, Arsha Nagrani, Shyamal Buch, Prateek Jain, Anurag Arnab, Sujoy Paul</div><div class="mb-2 flex flex-wrap gap-2 items-center"><span class="text-blue-300 font-bold">NeurIPS 2024</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Vision</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Efficiency</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Mixture of Experts</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Conditional Computation</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Selected Papers</span></div><div class="text-gray-300 mb-2"><b>TL;DR:</b> <!-- -->Token-wise routing between nested experts for tackling redundancy in visual modalities.</div><div class="flex flex-wrap gap-2 mb-2"><a href="https://arxiv.org/abs/2407.19985" class="font-bold text-white bg-gradient-to-r from-blue-600 to-blue-400 rounded-lg px-5 py-2 shadow hover:from-blue-700 hover:to-blue-500 transition" target="_blank" rel="noopener noreferrer">Read Paper</a><a href="https://github.com/yanndubs/invariant-self-supervised-learning" class="font-bold text-blue-300 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition" target="_blank" rel="noopener noreferrer">Code</a><a href="https://x.com/gaganjain1582/status/1820107343369035819" class="font-bold text-blue-400 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition" target="_blank" rel="noopener noreferrer">Twitter</a></div><div class="text-gray-400 text-xs">2024-05-22</div></div></div><div class="flex flex-wrap items-center gap-8 bg-gradient-to-br from-blue-900/60 to-yellow-900/40 rounded-2xl p-8 shadow-2xl border border-blue-400/20 backdrop-blur-md mb-8"><img alt="LookupViT: Compressing Visual Information to a smaller number of tokens teaser" loading="lazy" width="224" height="126" decoding="async" data-nimg="1" class="w-56 rounded-xl shadow-md mb-4 flex-shrink-0 border-2 border-blue-400/30" style="color:transparent" src="/lookupvit.png"/><div class="flex-1 min-w-[220px]"><div class="text-blue-300 font-extrabold text-xl mb-1">LookupViT: Compressing Visual Information to a smaller number of tokens</div><div class="text-gray-200 font-semibold mb-1">Rajat Koner, Gagan Jain, Prateek Jain, Volker Tresp, Sujoy Paul</div><div class="mb-2 flex flex-wrap gap-2 items-center"><span class="text-blue-300 font-bold">ECCV 2024</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Vision</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Efficiency</span><span class="bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold">Selected Papers</span></div><div class="text-gray-300 mb-2"><b>TL;DR:</b> <!-- -->An asyncronous version of attention with sub-quadratic scaling and superior performance.</div><div class="flex flex-wrap gap-2 mb-2"><a href="https://arxiv.org/abs/2407.12753" class="font-bold text-white bg-gradient-to-r from-blue-600 to-blue-400 rounded-lg px-5 py-2 shadow hover:from-blue-700 hover:to-blue-500 transition" target="_blank" rel="noopener noreferrer">Read Paper</a><a href="https://github.com/yanndubs/invariant-self-supervised-learning" class="font-bold text-blue-300 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition" target="_blank" rel="noopener noreferrer">Code</a><a href="https://x.com/gaganjain1582/status/1813951259222921629" class="font-bold text-blue-400 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition" target="_blank" rel="noopener noreferrer">Twitter</a></div><div class="text-gray-400 text-xs">2024-03-08</div></div></div></section></main><footer class="w-full text-center py-6 text-gray-400 text-sm bg-black/80 border-t border-white/10 mt-8 z-30 relative">© <!-- -->2025<!-- --> Gagan Jain. Powered by Next.js.</footer></div><!--$--><!--/$--><!--$--><!--/$--></div><script src="/_next/static/chunks/webpack-ca2f87e3a88850ff.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3494,[\"63\",\"static/chunks/63-817a1b282d253a1a.js\",\"177\",\"static/chunks/app/layout-fdd88cdb4d505173.js\"],\"default\"]\n3:I[7555,[],\"\"]\n4:I[1295,[],\"\"]\n5:I[3063,[\"63\",\"static/chunks/63-817a1b282d253a1a.js\",\"352\",\"static/chunks/app/publications/page-24d38f64d1e5a0e1.js\"],\"Image\"]\n6:I[9665,[],\"MetadataBoundary\"]\n8:I[9665,[],\"OutletBoundary\"]\nb:I[4911,[],\"AsyncMetadataOutlet\"]\nd:I[9665,[],\"ViewportBoundary\"]\nf:I[6614,[],\"\"]\n:HL[\"/_next/static/css/207c0470b180c9e7.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"HJnsTyJt2EwRAqM2dY0Vd\",\"p\":\"\",\"c\":[\"\",\"publications\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"publications\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/207c0470b180c9e7.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_5cfdac __variable_9a8899 antialiased\",\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"div\",null,{\"style\":{\"paddingTop\":\"90px\"},\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]}]]}],{\"children\":[\"publications\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"min-h-screen flex flex-col relative overflow-hidden bg-black\",\"children\":[[\"$\",\"main\",null,{\"className\":\"min-h-screen flex flex-col items-center justify-start bg-black/90 px-4 pt-32 pb-16\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-5xl font-extrabold text-blue-400 mb-10 tracking-tight\",\"children\":\"All Publications\"}],[\"$\",\"section\",null,{\"className\":\"w-full max-w-3xl mb-12 animate-fade-in-up delay-400\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"flex flex-wrap items-center gap-8 bg-gradient-to-br from-blue-900/60 to-yellow-900/40 rounded-2xl p-8 shadow-2xl border border-blue-400/20 backdrop-blur-md mb-8\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/fastgen.png\",\"alt\":\"Masked Generative Nested Transformers with Decode Time Scaling teaser\",\"width\":224,\"height\":126,\"className\":\"w-56 rounded-xl shadow-md mb-4 flex-shrink-0 border-2 border-blue-400/30\",\"unoptimized\":true}],[\"$\",\"div\",null,{\"className\":\"flex-1 min-w-[220px]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-blue-300 font-extrabold text-xl mb-1\",\"children\":\"Masked Generative Nested Transformers with Decode Time Scaling\"}],[\"$\",\"div\",null,{\"className\":\"text-gray-200 font-semibold mb-1\",\"children\":\"Sahil Goyal, Debapriya Tula, Pradeep Shenoy, Prateek Jain, Sujoy Paul\"}],[\"$\",\"div\",null,{\"className\":\"mb-2 flex flex-wrap gap-2 items-center\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-blue-300 font-bold\",\"children\":\"ICML 2025\"}],[[\"$\",\"span\",\"Vision\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Vision\"}],[\"$\",\"span\",\"Efficiency\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Efficiency\"}],[\"$\",\"span\",\"Conditional Computation\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Conditional Computation\"}],[\"$\",\"span\",\"Selected Papers\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Selected Papers\"}]]]}],[\"$\",\"div\",null,{\"className\":\"text-gray-300 mb-2\",\"children\":[[\"$\",\"b\",null,{\"children\":\"TL;DR:\"}],\" \",\"An efficient framework for progressive decoding with nested models for faster inference.\"]}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 mb-2\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2407.19985\",\"className\":\"font-bold text-white bg-gradient-to-r from-blue-600 to-blue-400 rounded-lg px-5 py-2 shadow hover:from-blue-700 hover:to-blue-500 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Read Paper\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/yanndubs/invariant-self-supervised-learning\",\"className\":\"font-bold text-blue-300 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Code\"}],[\"$\",\"a\",null,{\"href\":\"https://x.com/gaganjain1582/status/1820107343369035819\",\"className\":\"font-bold text-blue-400 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Twitter\"}]]}],[\"$\",\"div\",null,{\"className\":\"text-gray-400 text-xs\",\"children\":\"2024-11-15\"}]]}]]}],[\"$\",\"div\",\"1\",{\"className\":\"flex flex-wrap items-center gap-8 bg-gradient-to-br from-blue-900/60 to-yellow-900/40 rounded-2xl p-8 shadow-2xl border border-blue-400/20 backdrop-blur-md mb-8\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/eluder.png\",\"alt\":\"Bayesian Collaborative Bandits with Thompson Sampling for Improved Outreach in Maternal Health teaser\",\"width\":224,\"height\":126,\"className\":\"w-56 rounded-xl shadow-md mb-4 flex-shrink-0 border-2 border-blue-400/30\",\"unoptimized\":true}],[\"$\",\"div\",null,{\"className\":\"flex-1 min-w-[220px]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-blue-300 font-extrabold text-xl mb-1\",\"children\":\"Bayesian Collaborative Bandits with Thompson Sampling for Improved Outreach in Maternal Health\"}],[\"$\",\"div\",null,{\"className\":\"text-gray-200 font-semibold mb-1\",\"children\":\"Arpan Dasgupta, Arun Suggala, Karthikeyan Shanmugam, Milind Tambe, Aparna Taneja\"}],[\"$\",\"div\",null,{\"className\":\"mb-2 flex flex-wrap gap-2 items-center\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-blue-300 font-bold\",\"children\":\"AAMAS 2025\"}],[[\"$\",\"span\",\"Reinforcement Learning\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Reinforcement Learning\"}],[\"$\",\"span\",\"Theory\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Theory\"}],[\"$\",\"span\",\"Societal Impact\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Societal Impact\"}],[\"$\",\"span\",\"Selected Papers\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Selected Papers\"}]]]}],[\"$\",\"div\",null,{\"className\":\"text-gray-300 mb-2\",\"children\":[[\"$\",\"b\",null,{\"children\":\"TL;DR:\"}],\" \",\"An improved collaborative bandits approach with bayesian regret derivation for a special case!\"]}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 mb-2\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2410.21405\",\"className\":\"font-bold text-white bg-gradient-to-r from-blue-600 to-blue-400 rounded-lg px-5 py-2 shadow hover:from-blue-700 hover:to-blue-500 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Read Paper\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/yanndubs/invariant-self-supervised-learning\",\"className\":\"font-bold text-blue-300 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Code\"}],[\"$\",\"a\",null,{\"href\":\"https://x.com/gaganjain1582/status/1820107343369035819\",\"className\":\"font-bold text-blue-400 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Twitter\"}]]}],[\"$\",\"div\",null,{\"className\":\"text-gray-400 text-xs\",\"children\":\"2024-10-11\"}]]}]]}],[\"$\",\"div\",\"2\",{\"className\":\"flex flex-wrap items-center gap-8 bg-gradient-to-br from-blue-900/60 to-yellow-900/40 rounded-2xl p-8 shadow-2xl border border-blue-400/20 backdrop-blur-md mb-8\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/mone.png\",\"alt\":\"Mixture of Nested Experts: Adaptive Processing of Visual Tokens teaser\",\"width\":224,\"height\":126,\"className\":\"w-56 rounded-xl shadow-md mb-4 flex-shrink-0 border-2 border-blue-400/30\",\"unoptimized\":true}],[\"$\",\"div\",null,{\"className\":\"flex-1 min-w-[220px]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-blue-300 font-extrabold text-xl mb-1\",\"children\":\"Mixture of Nested Experts: Adaptive Processing of Visual Tokens\"}],[\"$\",\"div\",null,{\"className\":\"text-gray-200 font-semibold mb-1\",\"children\":\"Nidhi Hegde, Aditya Kusupati, Arsha Nagrani, Shyamal Buch, Prateek Jain, Anurag Arnab, Sujoy Paul\"}],[\"$\",\"div\",null,{\"className\":\"mb-2 flex flex-wrap gap-2 items-center\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-blue-300 font-bold\",\"children\":\"NeurIPS 2024\"}],[[\"$\",\"span\",\"Vision\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Vision\"}],[\"$\",\"span\",\"Efficiency\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Efficiency\"}],[\"$\",\"span\",\"Mixture of Experts\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Mixture of Experts\"}],[\"$\",\"span\",\"Conditional Computation\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Conditional Computation\"}],[\"$\",\"span\",\"Selected Papers\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Selected Papers\"}]]]}],[\"$\",\"div\",null,{\"className\":\"text-gray-300 mb-2\",\"children\":[[\"$\",\"b\",null,{\"children\":\"TL;DR:\"}],\" \",\"Token-wise routing between nested experts for tackling redundancy in visual modalities.\"]}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 mb-2\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2407.19985\",\"className\":\"font-bold text-white bg-gradient-to-r from-blue-600 to-blue-400 rounded-lg px-5 py-2 shadow hover:from-blue-700 hover:to-blue-500 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Read Paper\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/yanndubs/invariant-self-supervised-learning\",\"className\":\"font-bold text-blue-300 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Code\"}],[\"$\",\"a\",null,{\"href\":\"https://x.com/gaganjain1582/status/1820107343369035819\",\"className\":\"font-bold text-blue-400 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Twitter\"}]]}],[\"$\",\"div\",null,{\"className\":\"text-gray-400 text-xs\",\"children\":\"2024-05-22\"}]]}]]}],[\"$\",\"div\",\"3\",{\"className\":\"flex flex-wrap items-center gap-8 bg-gradient-to-br from-blue-900/60 to-yellow-900/40 rounded-2xl p-8 shadow-2xl border border-blue-400/20 backdrop-blur-md mb-8\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/lookupvit.png\",\"alt\":\"LookupViT: Compressing Visual Information to a smaller number of tokens teaser\",\"width\":224,\"height\":126,\"className\":\"w-56 rounded-xl shadow-md mb-4 flex-shrink-0 border-2 border-blue-400/30\",\"unoptimized\":true}],[\"$\",\"div\",null,{\"className\":\"flex-1 min-w-[220px]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-blue-300 font-extrabold text-xl mb-1\",\"children\":\"LookupViT: Compressing Visual Information to a smaller number of tokens\"}],[\"$\",\"div\",null,{\"className\":\"text-gray-200 font-semibold mb-1\",\"children\":\"Rajat Koner, Gagan Jain, Prateek Jain, Volker Tresp, Sujoy Paul\"}],[\"$\",\"div\",null,{\"className\":\"mb-2 flex flex-wrap gap-2 items-center\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-blue-300 font-bold\",\"children\":\"ECCV 2024\"}],[[\"$\",\"span\",\"Vision\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Vision\"}],[\"$\",\"span\",\"Efficiency\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Efficiency\"}],[\"$\",\"span\",\"Selected Papers\",{\"className\":\"bg-blue-900/40 text-blue-200 rounded-lg px-3 py-1 text-sm font-semibold\",\"children\":\"Selected Papers\"}]]]}],[\"$\",\"div\",null,{\"className\":\"text-gray-300 mb-2\",\"children\":[[\"$\",\"b\",null,{\"children\":\"TL;DR:\"}],\" \",\"An asyncronous version of attention with sub-quadratic scaling and superior performance.\"]}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 mb-2\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2407.12753\",\"className\":\"font-bold text-white bg-gradient-to-r from-blue-600 to-blue-400 rounded-lg px-5 py-2 shadow hover:from-blue-700 hover:to-blue-500 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Read Paper\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/yanndubs/invariant-self-supervised-learning\",\"className\":\"font-bold text-blue-300 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Code\"}],[\"$\",\"a\",null,{\"href\":\"https://x.com/gaganjain1582/status/1813951259222921629\",\"className\":\"font-bold text-blue-400 bg-blue-900/40 rounded-lg px-5 py-2 shadow hover:bg-blue-800/60 transition\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Twitter\"}]]}],[\"$\",\"div\",null,{\"className\":\"text-gray-400 text-xs\",\"children\":\"2024-03-08\"}]]}]]}]]}]]}],[\"$\",\"footer\",null,{\"className\":\"w-full text-center py-6 text-gray-400 text-sm bg-black/80 border-t border-white/10 mt-8 z-30 relative\",\"children\":[\"© \",2025,\" Gagan Jain. Powered by Next.js.\"]}]]}],[\"$\",\"$L6\",null,{\"children\":\"$L7\"}],null,[\"$\",\"$L8\",null,{\"children\":[\"$L9\",\"$La\",[\"$\",\"$Lb\",null,{\"promise\":\"$@c\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"cp_yDnEnRiyB1F46MWFsA\",{\"children\":[[\"$\",\"$Ld\",null,{\"children\":\"$Le\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$f\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"10:\"$Sreact.suspense\"\n11:I[4911,[],\"AsyncMetadata\"]\n7:[\"$\",\"$10\",null,{\"fallback\":null,\"children\":[\"$\",\"$L11\",null,{\"promise\":\"$@12\"}]}]\n"])</script><script>self.__next_f.push([1,"a:null\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n9:null\n"])</script><script>self.__next_f.push([1,"12:{\"metadata\":[],\"error\":null,\"digest\":\"$undefined\"}\nc:{\"metadata\":\"$12:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>